{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk import word_tokenize\n",
    "\n",
    "with open(\"data/train_full.json\") as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # Bx50\n",
    "        self.word_embeddings = nn.Embedding(10035, 50)\n",
    "        # Bx10\n",
    "        self.user_bot_embeddings = nn.Embedding(4, 10)\n",
    "        self.rnn = nn.RNN(60, 128, 1)\n",
    "        self.linear = nn.Linear(128, 3)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, 1, 128))\n",
    "    \n",
    "    # input => Bx2xN, B - sentence len\n",
    "    def forward(self, input, calc_softmax=False):\n",
    "        word_emb = self.word_embeddings(input[:, 0, :])\n",
    "        user_bot_emb = self.user_bot_embeddings(input[:, 1, :])\n",
    "        input_combined = torch.cat((word_emb, user_bot_emb), 2)\n",
    "        input_combined = input_combined.view(input_combined.size()[1], 1, input_combined.size()[-1])\n",
    "        \n",
    "        rnn_out, self.hidden = self.rnn(input_combined, self.hidden)\n",
    "        output = self.linear(self.hidden).view(1, 3)\n",
    "        \n",
    "        # Softmax только в самом конце считаем!\n",
    "        # Без батчей работаем пока\n",
    "        if calc_softmax:\n",
    "            probs = self.softmax(output)\n",
    "            return hidden, probs\n",
    "        else:\n",
    "            return hidden, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 2]), torch.Size([1, 1, 128]))"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "input = Variable(torch.LongTensor([[[1, 10], [1, 1]]]))\n",
    "hidden = model.init_hidden()\n",
    "\n",
    "input.size(), hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "\n",
       "Columns 0 to 18 \n",
       "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "\n",
       "Columns 19 to 37 \n",
       "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "\n",
       "Columns 38 to 56 \n",
       "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "\n",
       "Columns 57 to 75 \n",
       "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "\n",
       "Columns 76 to 94 \n",
       "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "\n",
       "Columns 95 to 113 \n",
       "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "\n",
       "Columns 114 to 127 \n",
       "    0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "[torch.FloatTensor of size 1x1x128]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden, output = model.forward(input)\n",
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mockup_data(vocab_words_size=100, vocab_user_bot_size=2):\n",
    "    # 5 dialogs\n",
    "    # 5 - 10 sentences each\n",
    "    dialogs = []\n",
    "    for _ in range(5):\n",
    "        dialog = []\n",
    "        for i in range(5, 10):\n",
    "            sent_words = torch.LongTensor(i).random_(vocab_words_size)\n",
    "            sent_userbot = torch.LongTensor(i).random_(vocab_user_bot_size)\n",
    "            dialog.append(torch.cat((sent_words, sent_userbot)).view(1, 2, i))\n",
    "        dialogs.append(dialog)\n",
    "    return dialogs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_dialogs_and_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        dialogs_vecs, labels = pickle.load(f)\n",
    "    labels = Variable(torch.LongTensor(labels))\n",
    "    dialogs = []\n",
    "    for dialog_vec in dialogs_vecs:\n",
    "        dialog = []\n",
    "        for sent_vec in dialog_vec:\n",
    "            dialog.append(torch.LongTensor(sent_vec).view(1, 2, -1))\n",
    "        dialogs.append(dialog)\n",
    "    return dialogs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs, labels = load_dialogs_and_labels('data/dilogs_and_labels.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dialogs = mockup_data()\n",
    "# labels = Variable(torch.LongTensor(len(dialogs)).random_(3))\n",
    "# labels, dialogs[0][0].size()\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Variable containing:\n",
      " 4.9176\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 5.2318\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    avg_loss = 0\n",
    "    for ind, dialog in enumerate(dialogs):\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        for sent in dialog[:-1]:\n",
    "            input = Variable(torch.LongTensor(sent))\n",
    "            hidden, out = model(input)\n",
    "        input = Variable(torch.LongTensor(dialog[-1]))\n",
    "        hidden, out = model(input, True)\n",
    "\n",
    "        loss = loss_function(out, labels[ind])\n",
    "        avg_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Loss: {}\".format(avg_loss / len(dialogs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222/11\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
